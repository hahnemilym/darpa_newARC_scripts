{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRIQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T22:25:10.654565Z",
     "start_time": "2018-04-03T22:25:10.484854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from subprocess import check_output, call\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from utils import exclude_subjects, select_subjects\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "data_dict_fname = os.getcwd() + '/data_dict.npz'\n",
    "\n",
    "task = 'ARC'\n",
    "fsfast_path = os.getcwd() + '/data/derivatives/fmri_first_levels'\n",
    "fmriprep_path = os.getcwd() + '/data/derivatives/fmriprep'\n",
    "user_id = '4190106' #type id in a terminal user is your user group is the last one (iliad)\n",
    "group_id = '1047'\n",
    "data_dir = '/space/lilli/3/users/DARPA-TRANSFER/mri'\n",
    "fs_license = '/autofs/space/karima_001/users/alex/software/license.txt'\n",
    "\n",
    "if not os.path.exists(fsfast_path):\n",
    "    os.makedirs(fsfast_path)\n",
    "if not os.path.exists(fmriprep_path):\n",
    "    os.makedirs(fmriprep_path)\n",
    "\n",
    "# absolute paths needed for docker\n",
    "root = '/autofs/space/karima_004/users/learning_mri/'\n",
    "data_path = root + 'data'\n",
    "mriqc_path = '%s/derivatives/mri/mriqc' % data_path\n",
    "\n",
    "# extract mri subjects \n",
    "info = pd.read_csv(root + 'mri_subjects.csv')\n",
    "info = info.loc[info[task.title()], 'Subject'].reset_index()\n",
    "subjects = list(info.Subject)\n",
    "with open('%s/subjects' % fsfast_path, 'w') as fid:\n",
    "    for s in subjects:\n",
    "        fid.write(s + '\\n')\n",
    "        \n",
    "np.savez_compressed(data_dict_fname,task=task,fsfast_path=fsfast_path,fmriprep_path=fmriprep_path,\n",
    "                    user_id=user_id,group_id=group_id,data_dir=data_dir,root=root,data_path=data_path,\n",
    "                    mriqc_path=mriqc_path,subjects=subjects,fs_license=fs_license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T21:00:04.568936Z",
     "start_time": "2018-04-03T20:59:49.295450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mriqc v0.10.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(check_output(['docker',\n",
    "                    'run', \n",
    "                    'poldracklab/mriqc:latest',\n",
    "                    '--version']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subject Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T21:02:15.404439Z",
     "start_time": "2018-04-03T21:02:09.381035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4190106\n",
      "docker run --rm -u 4190106:1047 -v /autofs/space/karima_004/users/learning_mri/data:/data:ro -v /autofs/space/karima_004/users/learning_mri/data/derivatives/mri/mriqc:/out -v /autofs/space/karima_004/users/learning_mri/data/derivatives/mri/mriqc:/work poldracklab/mriqc:latest /data /out participant -w /work --no-sub --verbose-reports --write-graph --ica --n_procs=10 --fft-spikes-detector --fd_thres 0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-86b836aab131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocker_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocker_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eintr_retry_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mECHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_eintr_retry_call\u001b[0;34m(func, *args)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dict = np.load(os.getcwd() + '/data_dict.npz')\n",
    "user_id, group_id, mriqc_path = data_dict['user_id'].item(),data_dict['group_id'].item(),data_dict['mriqc_path'].item()\n",
    "docker_command = ['docker',\n",
    "                  'run',\n",
    "                  '--rm',\n",
    "                  '-u', user_id + ':' + group_id,\n",
    "                  '-v', '%s:/data:ro' % data_path,\n",
    "                  '-v', '%s:/out' % mriqc_path,\n",
    "                  '-v', '%s:/work' % mriqc_path,\n",
    "                  'poldracklab/mriqc:latest',\n",
    "                  '/data', '/out',\n",
    "                  'participant',\n",
    "                  '-w', '/work',\n",
    "                  '--no-sub',\n",
    "                  '--verbose-reports',\n",
    "                  '--write-graph',\n",
    "                  '--ica',\n",
    "                  '--n_procs=10',\n",
    "                  '--fft-spikes-detector',\n",
    "                  '--fd_thres', '0.9']\n",
    "\n",
    "folders = ['anatMRIQCT1w','derivatives','funcMRIQC','logs','reports','workflow_enumerator']\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(mriqc_path + '/' + folder):\n",
    "        os.makedirs(mriqc_path + '/' + folder)\n",
    "        \n",
    "print(' '.join(docker_command))\n",
    "call(docker_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Group Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T22:20:01.741888Z",
     "start_time": "2018-04-03T22:19:57.919989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker run --rm -u 4190106:1047 -v /autofs/space/karima_004/users/learning_mri/data:/data:ro -v /autofs/space/karima_004/users/learning_mri/data/derivatives/mri/mriqc:/out -v /autofs/space/karima_004/users/learning_mri/data/derivatives/mri/mriqc:/work poldracklab/mriqc:latest /data /out group -w /work\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d181a14cdb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                   '-w', '/work']\n\u001b[1;32m     14\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocker_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocker_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eintr_retry_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mECHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_eintr_retry_call\u001b[0;34m(func, *args)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dict = np.load(os.getcwd() + '/data_dict.npz')\n",
    "user_id, group_id, mriqc_path = data_dict['user_id'].item(),data_dict['group_id'].item(),data_dict['mriqc_path'].item()\n",
    "docker_command = ['docker',\n",
    "                  'run',\n",
    "                  '--rm',\n",
    "                  '-u', user_id + ':' + group_id,\n",
    "                  '-v', '%s:/data:ro' % data_path,\n",
    "                  '-v', '%s:/out' % mriqc_path,\n",
    "                  '-v', '%s:/work' % mriqc_path,\n",
    "                  'poldracklab/mriqc:latest',\n",
    "                  '/data', '/out',\n",
    "                  'group',\n",
    "                  '-w', '/work']\n",
    "print(' '.join(docker_command))\n",
    "call(docker_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observations & Subject Exclusions\n",
    "\n",
    "We use the group reports for the T1 and BOLD scans to determine exclusions:\n",
    "\n",
    "- <a href=\"./data/derivatives/mri/mriqc/reports/T1w_group.html\"> T1 Group Report</a>\n",
    "- <a href=\"./data/derivatives/mri/mriqc/reports/bold_group.html\"> BOLD Group Report</a>\n",
    "\n",
    "T1 Observations:\n",
    "* pre-scan normalize was off for the T1 on the bay 4 prisma\n",
    "* The newer prisma has much better noise quality than the older trio scanners as measured by EFC (measure of ghosting/blurring) & FBER (relative energy within brain relative to background). Hard to actually see this however due to lack of pre-scan normalize for the prisma scans. This difference also appears whenever anything is normalized by background noise (such as SNRD).\n",
    "* The CJV & CNR measures appeared to be inversely related with heavy tails. The tails seemed ok and not grounds for exclusion. It seems it may have been sensitive to the amount of gyral folding and hence the discriminability of grey and white matter?\n",
    "* A few other outliers, but most related to wrap around, ghosting, etc. that did not affect the brain itself (apart from exclusions below).\n",
    "\n",
    "BOLD Observations:\n",
    "* A few with fairly bad motion. Leaving in to see if scrubbing/correction can help.\n",
    "* sub-hc045 has weird frontal dropout (not present in T1). Leaving in to see if b0 correction helps. Outlier on FWHM y.\n",
    "* sub-hc041 lots of motion may be able to censor\n",
    "\n",
    "T1 Observations:\n",
    "* sub-hc034 spiking, likely poor recon\n",
    "\n",
    "Subject Exclusions:\n",
    "* sub-pp004: Motion\n",
    "* sub-ep011: Motion\n",
    "* sub-hc037: Appears to not have had anterior head coil in. Caught by being outlier in FWHM y for T1 and bold.\n",
    "* sub-hc020: Extremely bad motion. Caught as outlier on AOR and Average FD.\n",
    "* sub-hc047: Really bad motion and really bad distortion/dropout in the frontal regions. Caught as outlier on Average FD and FWHM y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "subjects = ['hc020', 'pp004', 'hc037', 'hc047','ep011']\n",
    "\n",
    "f = os.getcwd() + '/mri_subjects.csv'\n",
    "sub_info = pd.read_csv(f)\n",
    "if 'exclude' not in sub_info:\n",
    "    sub_info['exclude'] = False\n",
    "sub_info.loc[sub_info.Subject.isin(subjects),'exclude'] = True\n",
    "sub_info.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with FMRIPREP\n",
    "\n",
    "Here we use the Poldrack lab's <a href=\"https://fmriprep.readthedocs.io/en/1.0.0-rc2/\">fmriprep v1.0.0-rc2</a> software package to perform preprocessing of the BOLD data. fmriprep is an awesome tool built off of nipype that combines different preprocessing steps across multiple packages into a single preprocessing workflow. \n",
    "\n",
    "\n",
    "The full workflow is detailed <a href=\"https://fmriprep.readthedocs.io/en/1.0.0-rc2/workflows.html#\">here</a>. The workflow does many things including generating freesurfer reconstructions, motion correction, beta0 field correction, slice time correction, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T21:08:15.014836Z",
     "start_time": "2018-04-03T21:08:05.442347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fmriprep v1.0.6-2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from subprocess import check_output, call\n",
    "\n",
    "print(check_output(['docker',\n",
    "                    'run', \n",
    "                    'poldracklab/fmriprep:latest',\n",
    "                    '--version']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fmriprep\n",
    "\n",
    "\n",
    "We run the fmriprep command via the wrapper script run_fmriprep.py. This is a python wrapper for generating the docker run command in order to run fmriprep. The command for each subject looks as follows: \n",
    "\n",
    "    docker run --rm -v $data_path:/data:ro -v $fmriprep_path:/out -v $fmriprep_path:/work -v $FS_LICENSE:/opt/freesurfer/license.txt -w /work poldracklab/fmriprep:latest /data /out participant --participant_label $subject -t $task -w /work --omp-nthreads 1 --nthreads 1 --output-space fsaverage fsnative T1w template \n",
    "    \n",
    "This command runs the fmriprep command with the bids data folder (./data) as input and it stores its output in the ./data/derivatives folder. We transform the preprocessed functional data into both the native surface (fsnative) and T1 (T1w) space as well as normalized fsaverage and MNI template spaces. The write graph option writes out the computation graph which is pictured below.\n",
    "\n",
    "Parallelization Note: I explicitly set the number of threads for each fmriprep call to 1. \n",
    "Instead, I parallelize across subjects by using the subprocess.Popen command to start a new process for each separate subject. Without parallelization, preprocessing would take ~60 days (1 day per subject), but I was able to make use of 15 cores at a time on my computer which reduced the processing time to ~4 days since I could thus process 15 subjects at a time. Trying to run all subjects at once with fmriprep with the number of threads set greater than 1 did not seem to actually parallelize across subjects and was much slower. Note that the memory usage gets quite intense for this (~30 GB for 15 subjects).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T22:25:20.840112Z",
     "start_time": "2018-04-03T22:25:20.814504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker run --rm -u 4190106:1047 -v /autofs/space/karima_004/users/learning_mri/data:/data:ro -v /autofs/space/karima_004/users/learning_mri/data/derivatives/fmriprep:/out -v /autofs/space/karima_004/users/learning_mri/data/derivatives/fmriprep:/work -v /autofs/space/karima_001/users/alex/software/license.txt:/opt/freesurfer/license.txt -w /work poldracklab/fmriprep:latest /data /out participant --participant_label sub-ep011 -t Learning -w /work --omp-nthreads 1 --nthreads 1 --output-space fsaverage fsnative T1w template\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_dict = np.load(os.getcwd() + '/data_dict.npz')\n",
    "task, subjects, fmriprep_path,fs_license,data_path = data_dict['task'].item(),data_dict['subjects'],data_dict['fmriprep_path'].item(),data_dict['fs_license'].item(),data_dict['data_path'].item()\n",
    "user_id,group_id = data_dict['user_id'].item(),data_dict['group_id'].item()\n",
    "import sys\n",
    "from subprocess import check_output, call\n",
    "\n",
    "docker_command = ['docker', 'run', '--rm',\n",
    "                  '-u', '%s:%s' %(user_id, group_id),\n",
    "                  '-v', '%s:/data:ro' % data_path,\n",
    "                  '-v', '%s:/out' % fmriprep_path,\n",
    "                  '-v', '%s:/work' % fmriprep_path,\n",
    "                  '-v', '%s:/opt/freesurfer/license.txt' %(fs_license),\n",
    "                  '-w', '/work', 'poldracklab/fmriprep:latest',\n",
    "                  '/data', '/out','participant',\n",
    "                  '--participant_label','sub-ep011',\n",
    "                  '-t', task,\n",
    "                  '-w', '/work',\n",
    "                  '--omp-nthreads', '1',\n",
    "                  '--nthreads', '1',\n",
    "                  '--output-space', 'fsaverage', 'fsnative', 'T1w', 'template'\n",
    "                  ]\n",
    "\n",
    "print(' '.join(docker_command))\n",
    "#call(docker_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Compatible with FSFAST for 1st levels\n",
    "\n",
    "To do this, we need to transfer data into correct hierarchy (T1 sampled data and mask). Then run preproc-sess with spatial resampling and smoothing and everything else checked no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make FSFAST Directory & Transfer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T20:52:16.156220Z",
     "start_time": "2018-04-04T20:51:18.556825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep006\n",
      "ep008\n",
      "ep009\n",
      "ep011\n",
      "hc001\n",
      "hc003\n",
      "hc004\n",
      "hc005\n",
      "hc006\n",
      "hc007\n",
      "hc008\n",
      "hc009\n",
      "hc010\n",
      "hc011\n",
      "hc012\n",
      "hc013\n",
      "hc015\n",
      "hc016\n",
      "hc017\n",
      "hc018\n",
      "hc019\n",
      "hc020\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/autofs/space/karima_004/users/learning_mri/data/derivatives/fmriprep/fmriprep/sub-hc020/func/sub-hc020_task-Learning_bold_space-T1w_preproc.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ec4c42899ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s/%s/00%i/fmcpr.nii.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     shutil.copy(src % (fmriprep_path, sub, sub, task.title(), 'preproc'),\n\u001b[0;32m---> 35\u001b[0;31m                 dest1 % (fsfast_path, sub, task, task_day))\n\u001b[0m\u001b[1;32m     36\u001b[0m     shutil.copy(src % (fmriprep_path, sub, sub, task.title(), 'preproc'),\n\u001b[1;32m     37\u001b[0m                 dest2 % (fsfast_path, sub, task, task_day))\n",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSpecialFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`%s` is a named pipe\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/autofs/space/karima_004/users/learning_mri/data/derivatives/fmriprep/fmriprep/sub-hc020/func/sub-hc020_task-Learning_bold_space-T1w_preproc.nii.gz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "data_dict = np.load(os.getcwd() + '/data_dict.npz')\n",
    "task, subjects, fsfast_path,data_dir = data_dict['task'].item(),data_dict['subjects'],data_dict['fsfast_path'].item(),data_dict['data_dir'].item()\n",
    "                \n",
    "for sub in subjects:\n",
    "    print(sub)\n",
    "    # find day info\n",
    "    for i in range(1,4):\n",
    "        fname = '%s/%s_0%i/cfg.txt' % (data_dir, sub.upper(),i)\n",
    "        if os.path.isfile(fname):\n",
    "            cfg = pd.read_csv(fname,sep='\\s+')\n",
    "            if any([task.title() in scan for scan in cfg.ix[:,-1]]):\n",
    "                task_day = i      \n",
    "    # make the subject folder hierarchy\n",
    "    sub_dir = '%s/%s/%s/00%i/masks' % (fsfast_path, sub, task, task_day)\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "    sub_dir = '%s/%s/%s/masks' % (fsfast_path, sub, task)\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.makedirs(sub_dir)\n",
    "    # add a fsfast subject name indicator\n",
    "    with open('%s/%s/subjectname' % (fsfast_path, sub), 'w') as fid:\n",
    "        fid.write(sub)\n",
    "    # copy over the bold data\n",
    "    # copied as f and fmcpr because I have to trick fsfast since it doesn't\n",
    "    # seem to actually support skipping motion correction\n",
    "    src = '%s/fmriprep/sub-%s/func/sub-%s_task-%s_bold_space-T1w_%s.nii.gz'\n",
    "    dest1 = '%s/%s/%s/00%i/f.nii.gz' \n",
    "    dest2 = '%s/%s/%s/00%i/fmcpr.nii.gz' \n",
    "    shutil.copy(src % (fmriprep_path, sub, sub, task.title(), 'preproc'),\n",
    "                dest1 % (fsfast_path, sub, task, task_day))\n",
    "    shutil.copy(src % (fmriprep_path, sub, sub, task.title(), 'preproc'),\n",
    "                dest2 % (fsfast_path, sub, task, task_day))\n",
    "    # copy over the brainmask\n",
    "    dest1 = '%s/%s/%s/00%i/masks/brain.nii.gz' \n",
    "    dest2 = '%s/%s/%s/masks/brain.nii.gz' \n",
    "    shutil.copy(src % (fmriprep_path, sub, sub, task.title(),'brainmask'),\n",
    "                dest1 % (fsfast_path, sub, task, task_day))\n",
    "    shutil.copy(src % (fmriprep_path, sub, sub, task.title(),'brainmask'),\n",
    "                dest2 % (fsfast_path, sub, task))\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Normalization & Smoothing\n",
    "\n",
    "The two remaining steps beyond fmriprep that we need to do are sample to common spatial spaces (fsaverage for surface and mni305 for subcortical volume) and then spatially smooth the signal. We re-sample to commons spaces so that we can compare across subjects (individual spaces wouldn't line up). We spatially smooth the resulting signals with gaussian kernels (fwhm 4 voxels). This value was determine at Nichol's recommendation from the tfce paper where optimal results were found with a spatial smoothing sigma of 1.5 voxels = 2.335 * 1.5 rounded to 4.\n",
    "\n",
    "Unfortunately, to play nicely with fsfast's first level analysis pipeline which I need (not so easy to implement the autocorrelation correction), I have to run a few other things. This includes re-registering the functional data to the T1. Since they are already aligned this process should be good and a sanity check below confirms that.\n",
    "\n",
    "The last output is I use fsfsast's mkbrainmask-sess to compute the global mean value using a temporary brainmask produced by fsfast. This value is used to normalize the images for input to the first level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "task=learning\n",
    "# Source freesurfer v6\n",
    "export FREESURFER_HOME=/usr/local/freesurfer/stable6_0_0\n",
    ". /usr/local/freesurfer/stable6_0_0/SetUpFreeSurfer.sh\n",
    "export SUBJECTS_DIR=/autofs/space/karima_001/users/alex/$task/data/derivatives/freesurfer\n",
    "\n",
    "# get into fsfast directory\n",
    "export FSFAST=./data/derivatives/fmri_first_levels\n",
    "cd $FSFAST\n",
    "\n",
    "# make the template run\n",
    "mktemplate-sess -sf subjects -d . -fsd $task -update\n",
    "\n",
    "# co-register bold to t1\n",
    "register-sess -sf subjects -d . -fsd $task -delete-dat -dof 6 -bold -per-run -update -init-coreg\n",
    "\n",
    "# Run here to compute global mean value (delete tmp brainmasks afterwards)\n",
    "mkbrainmask-sess -maskstem brainn -fsd $task -sf subjects -d . -per-run -update\n",
    "rm -r */$task/masks/brainn*\n",
    "rm -r */$task/001/masks/brainn*\n",
    "\n",
    "# sample to fsaverage surface\n",
    "rawfunc2surf-sess -fwhm 4 -sf subjects -d . -fsd $task -trgsubject fsaverage -update -per-run\n",
    "\n",
    "# sample to mni305 subcortical volume\n",
    "rawfunc2tal-sess -fwhm 4 -sf subjects -d . -fsd $task -update -subcort-mask -per-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check Registrations\n",
    "\n",
    "The bold had already been sampled to the T1 space, but this is just a nice sanity check to make sure nothing went wrong with the registration process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T20:47:49.808690Z",
     "start_time": "2018-04-03T20:47:49.355648Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "task = 'learning'\n",
    "\n",
    "fsfast_path = os.getcwd() + '/data/derivatives/fmri_first_levels'\n",
    "\n",
    "info = pd.read_csv(os.getcwd() + '/mri_subjects.csv')\n",
    "info = info.loc[info[task.title()], 'Subject'].reset_index()\n",
    "subjects = list(info.Subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T20:47:53.852447Z",
     "start_time": "2018-04-03T20:47:53.198073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: cound not find session ep001 ERROR: cound not find session ep001 ERROR: cound not find session ep002 ERROR: cound not find session ep002 ERROR: cound not find session ep004 ERROR: cound not find session ep005 ERROR: cound not find session ep005 ERROR: cound not find session ep007 ERROR: cound not find session ep012 ERROR: cound not find session hc002 ERROR: cound not find session hc002 ERROR: cound not find session hc014 ERROR: cound not find session hc014 ERROR: cound not find session hc024 ERROR: cound not find session hc024 ERROR: cound not find session hc040 ERROR: cound not find session pp013 ERROR: cound not find session pp013 ERROR: cound not find session sp001 ERROR: cound not find session sp001 ERROR: cound not find session sp002 ERROR: cound not find session sp002 ERROR: cound not find session sp003 ERROR: cound not find session sp003 ERROR: cound not find session sp006 ERROR: cound not find session sp006 ERROR: cound not find session sp007 ERROR: cound not find session sp007 ERROR: cound not find session sp009 ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep003 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep006 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep008 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep009 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep011 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc001 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc003 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc004 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc005 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc006 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc007 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc008 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc009 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc010 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc011 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc012 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc013 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc015 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc015 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc016 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc016 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc017 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc017 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc018 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc020 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc020 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc022 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc022 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc023 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc023 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc025 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc025 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc026 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc027 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc028 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc029 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc030 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc031 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc032 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc033 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc034 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc035 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc036 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp001 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp002 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp003 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp005 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp005 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp006 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp006 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp007 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp007 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp008 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp008 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp009 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp009 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp010 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp011 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp012 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp014 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp015 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp016 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp004 is specified multiple times ERROR: /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp005 is specified multiple times /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep003 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep003 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep011 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/ep011 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc001 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc001 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc003 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc003 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc007 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc007 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc010 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc010 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc011 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc011 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc012 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc012 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc013 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc013 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc015 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc015 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc015 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc016 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc016 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc016 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc017 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc017 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc017 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc018 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc018 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc019 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc020 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc020 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc020 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc021 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc022 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc022 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc022 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc023 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc023 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc023 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc025 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc025 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc025 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc026 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc026 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc027 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc027 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc028 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc028 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc029 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc029 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc030 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc030 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc031 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc031 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc032 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc032 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc033 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc033 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc034 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc034 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc035 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc035 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc036 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc036 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc037 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc038 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc041 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc042 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc044 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc045 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/hc047 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp001 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp001 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp002 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp002 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp003 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp003 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp006 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp007 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp007 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp007 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp008 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp009 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp010 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp010 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp011 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp011 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp012 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp012 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp014 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp014 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp015 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp015 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp016 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/pp016 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp004 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp005 /autofs/space/karima_004/users/learning_mri/data/derivatives/fmri_first_levels/sp008\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "task=learning\n",
    "# Source freesurfer v6\n",
    "export FREESURFER_HOME=/usr/local/freesurfer/stable6_0_0\n",
    ". /usr/local/freesurfer/stable6_0_0/SetUpFreeSurfer.sh\n",
    "export SUBJECTS_DIR=/autofs/space/karima_001/users/alex/$task/data/derivatives/freesurfer\n",
    "\n",
    "# get into fsfast directory\n",
    "export FSFAST=./data/derivatives/fmri_first_levels\n",
    "cd $FSFAST\n",
    "\n",
    "tkregister-sess -sf subjects -fsd $task -per-run -bbr-sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assemble Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T20:26:26.931950Z",
     "start_time": "2018-01-26T20:26:26.578591Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "root_dir = '/autofs/space/karima_004/users/learning_mri/data/'\n",
    "\n",
    "observed = 0.90    # 90% or more of possible data must be present\n",
    "accuracy = 0.25    # 25% accuracy or higher\n",
    "\n",
    "csv_files = sorted([f for f in os.listdir(root_dir) if f.endswith('mri-1')])\n",
    "\n",
    "if not os.path.isdir('fsfast'):\n",
    "    os.path.mkdirs('fsfast')\n",
    "with open('fsfast/sessid', 'w') as sessid:\n",
    "    for f in csv_files:\n",
    "        ## Get subject name.\n",
    "        subject = f.split('_')[0]\n",
    "        ## Load CSV.\n",
    "        df = read_csv(os.path.join(root_dir,f))\n",
    "        df = df[df.Condition != 0].reset_index(drop=True)   # Drop rest trials.\n",
    "        ## Assess quality.\n",
    "        if (df.ResponseAccuracy != 99).mean() < observed: continue\n",
    "        else: df = df[df.ResponseAccuracy != 99].reset_index(drop=True)\n",
    "        if df.ResponseAccuracy.mean() < accuracy: continue\n",
    "        else: sessid.write('%s\\n' %subject)\n",
    "    \n",
    "## Save.\n",
    "sessid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T22:16:43.120464Z",
     "start_time": "2018-01-26T22:16:35.355241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep006 ep008 hc001 hc003 hc005 hc006 hc007 hc008 hc009 hc010 hc011 hc012 hc013 hc015 hc016 hc017 hc019 hc020 hc021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/autofs/space/karima_001/users/alex/software/anaconda2.7/lib/python2.7/site-packages/ipykernel_launcher.py:106: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hc022 hc025 hc026 hc028 hc029 hc030 hc031 hc032 hc033 hc034 hc035 hc036 hc038 hc042 hc045 hc047 pp002 pp003 pp004 pp005 pp006 pp007 pp008 pp009 pp010 pp012 pp014 pp015 pp016 sp004 sp005 Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from pandas import read_csv\n",
    "from scipy.special import gammaln\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "root_dir = '/autofs/space/karima_001/users/alex/learning/data/'\n",
    "mri_dir = '/autofs/space/karima_001/users/alex/learning/data/'\n",
    "\n",
    "task = 'learning'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define useful functions.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def spm_hrf(RT, P=None, fMRI_T=16):\n",
    "    p = np.array([6, 16, 1, 1, 6, 0, 32], dtype=float)\n",
    "    if P is not None:\n",
    "        p[0:len(P)] = P\n",
    "    _spm_Gpdf = lambda x, h, l: np.exp(h * np.log(l) + (h - 1) * np.log(x) - (l * x) - gammaln(h))\n",
    "    # modelled hemodynamic response function - {mixture of Gammas}\n",
    "    dt = RT / float(fMRI_T)\n",
    "    u = np.arange(0, int(p[6] / dt + 1)) - p[5] / dt\n",
    "    with np.errstate(divide='ignore'):  # Known division-by-zero\n",
    "        hrf = _spm_Gpdf(u, p[0] / p[2], dt / p[2]) - _spm_Gpdf(u, p[1] / p[3],\n",
    "                                                               dt / p[3]) / p[4]\n",
    "    idx = np.arange(0, int((p[6] / RT) + 1)) * fMRI_T\n",
    "    hrf = hrf[idx]\n",
    "    hrf = hrf / np.sum(hrf)\n",
    "    return hrf\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "subjects = np.loadtxt('fsfast/sessid', dtype='str')\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print subject,\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Get params.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    bold_fname = mri_dir + 'sub-%s/func/sub-%s_task-%s_bold.nii' %(subject,subject,task.title())\n",
    "    img = nib.load(bold_fname) \n",
    "    v1,v2,v3,tr = img.header.get_zooms()\n",
    "    xshape,yshape,zshape,n_acq = img.get_data().shape\n",
    "    sfreq = float(zshape/tr)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Initialize regressors.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "\n",
    "    ## Setup timing information.\n",
    "    total_time = n_acq * tr\n",
    "    times = np.arange(0, total_time+1./sfreq, 1./sfreq)\n",
    "    n_times = times.shape[0]\n",
    "\n",
    "    ## Load behavior information.\n",
    "    df = read_csv(os.path.join(root_dir, 'sub-%s/mri/%s_%s_mri-1' %(subject,subject,task)))\n",
    "    df = df[df.Condition != 0]            # Drop rest trials.\n",
    "    df = df[df.ResponseAccuracy != 99]    # Drop missing trials.\n",
    "    \n",
    "    ## Define contrasts.\n",
    "    conditions = np.unique(df.Condition)\n",
    "    n_conditions = len(conditions)\n",
    "    \n",
    "    ## Initialize boxcars.\n",
    "    neural_signal = np.zeros((n_conditions,n_times))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Generate boxcars.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# \n",
    "    \n",
    "    for n in range(n_conditions):\n",
    "        ## Extract onsets/offsets.\n",
    "        onsets  = df.loc[df.Condition==n+1, 'StimOnset'].as_matrix()\n",
    "        offsets = df.loc[df.Condition==n+1, 'ResponseOnset'].as_matrix()\n",
    "        ## Round onsets/offsets.\n",
    "        onsets  = onsets.round(np.log10(sfreq).astype(int))\n",
    "        offsets = offsets.round(np.log10(sfreq).astype(int))   \n",
    "        ## Iteratively generate boxcars.\n",
    "        for onset, offset in zip(onsets,offsets): \n",
    "            mask = (times >= onset) & (times <= offset)\n",
    "            neural_signal[n,mask] = 1\n",
    "\n",
    "    ## Perform convolution.\n",
    "    hrf = spm_hrf(1./sfreq)\n",
    "    bold_signal = np.apply_along_axis(np.convolve, 1, neural_signal, v=hrf)\n",
    "    bold_signal = bold_signal[:,:neural_signal.shape[-1]] # Set back to original length.\n",
    "    \n",
    "    ## Downsample to start of TR.\n",
    "    tr_onsets = np.insert( np.cumsum( np.ones(n_acq-1)*tr ), 0, 0 )\n",
    "    ds = np.array([abs(tr_onsets-t).min() < 0.00001 for t in times])\n",
    "    if not ds.sum() == n_acq: raise ValueError('Oh noes!')\n",
    "    bold_signal = bold_signal[:,ds]\n",
    "    \n",
    "    ## Normalize regressors. [See Calhoun et al. (2004)]\n",
    "    ## First we normalize [Intercept, DDB, Valence] such that the sum\n",
    "    ## of their timeseries squared is equal to 1.\n",
    "    sums = np.power(bold_signal,2).sum(axis=1)\n",
    "    bold_signal = (bold_signal.T / np.sqrt(sums)).T\n",
    "    \n",
    "    ## Save task regressors.\n",
    "    for arr, label in zip(bold_signal, conditions):\n",
    "        fdir = mri_dir + 'sub-%s/%s_001/' %(subject,task)\n",
    "        if not os.path.isdir(fdir):\n",
    "            os.makedirs(fdir)\n",
    "        f = fdir + '%s.par' %(label)\n",
    "        np.savetxt(f, arr[:,np.newaxis], fmt='%s')\n",
    "        \n",
    "    np.savez(fdir + 'params.npz',tr=tr,sfreq=sfreq,n_acq=n_acq,tr_onsets=tr_onsets)\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Time Point Censors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from scipy.signal import detrend\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "mri_dir = '/autofs/space/karima_001/users/alex/learning/data/'\n",
    "task = 'learning'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Scrubbing parameters.\n",
    "thresholds = [0.0, 0.5, 1.0]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Get subjects list.\n",
    "subjects = np.loadtxt('fsfast/sessid', dtype='str')\n",
    "with open('fmri/nuisance_info.csv','w') as info:\n",
    "    info.write('Subject,n_mc,FD=0.0,FD=0.5,FD=1.0\\n')\n",
    "    for subject in subjects:\n",
    "        info.write('%s,' %subject)\n",
    "        f = np.load(mri_dir + 'sub-%s/%s_001/params.npz' %(subject,task))\n",
    "        tr,sfreq,n_acq,tr_onsets = f['tr'].item(),f['sfreq'].item(),f['n_acq'].item(),f['tr_onsets']\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute framewise displacement.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Read motion data.\n",
    "        mc = os.path.join(mri_dir, subject, 'msit_001', '001', 'fmcpr.mcdat')\n",
    "        try: mc = np.loadtxt(mc)[:,1:7]\n",
    "        except IOError: \n",
    "            print 'Drop %s.' %subject\n",
    "            continue\n",
    "\n",
    "        ## Invert angular displacement.\n",
    "        fd = mc.copy()\n",
    "        fd[:,:3] = np.deg2rad(fd[:,:3]) \n",
    "        fd[:,:3] *= 50\n",
    "\n",
    "        ## Compute framewise displacement (See Power 2012, 2014).\n",
    "        fd = np.insert( np.abs( np.diff(fd, axis=0) ).sum(axis=1), 0, 0 )\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Compute motion regressors.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        ## Remove trends.\n",
    "        mc = detrend(mc, axis=0, type='constant')\n",
    "        mc = detrend(mc, axis=0, type='linear')\n",
    "\n",
    "        ## Perform PCA.\n",
    "        pca = PCA(n_components=6)\n",
    "        mc = pca.fit_transform(mc)\n",
    "\n",
    "        ## Take only the number of components explaining 90% of the variance.\n",
    "        varexp = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(varexp >= 0.9) + 1\n",
    "        mc = mc[:,:n_components]\n",
    "\n",
    "        ## Save motion regressor.\n",
    "        f = '%s/%s/msit_001/001/afMSIT.mc.par' %(mri_dir,subject)\n",
    "        np.savetxt(f, mc, fmt='%s')\n",
    "        info.write('%s,' %n_components)\n",
    "\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        ### Write scrubbers.\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            ## Find threshold violations.\n",
    "            if not threshold: ix, = np.where(fd >= np.inf)\n",
    "            else: ix, = np.where(fd >= threshold)\n",
    "\n",
    "            ## Save.\n",
    "            info.write('%s,' %len(ix))\n",
    "            f = '%s/%s/msit_001/001/afMSIT.censor.%s.par' %(mri_dir,subject,threshold)\n",
    "            if len(ix): np.savetxt(f, tr_onsets[ix,np.newaxis], fmt='%s')\n",
    "\n",
    "        info.write('\\n')\n",
    "\n",
    "    info.close()\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Beta Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "mri_dir = '/space/lilli/4/users/DARPA-MSIT'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "subjects = np.loadtxt('scripts/fsfast/sessid.manual', dtype='str')\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print subject,\n",
    "    \n",
    "    for hemi in ['lh','rh']:\n",
    "        \n",
    "        ## Load data.\n",
    "        f = os.path.join(mri_dir, subject, 'msit_001', '001', 'fmcpr.sm6.fsaverage.%s.b0dc.nii.gz' %hemi)\n",
    "        obj = nib.load(f)\n",
    "        \n",
    "        ## Extract data and average over acquisitions.\n",
    "        data = obj.get_data()\n",
    "        data = np.apply_over_axes(np.mean, data, -1)\n",
    "        \n",
    "        ## Save.\n",
    "        f = os.path.join(mri_dir, subject, 'msit_001', 'afMSIT.6.0.5.%s' %hemi, 'betaconstant.nii.gz')\n",
    "        obj = nib.Nifti1Image(data, obj.affine)\n",
    "        nib.save(obj, f)\n",
    "        \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Percent Signal Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "mri_dir = '/space/lilli/4/users/DARPA-MSIT'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "subjects = np.loadtxt('scripts/fsfast/sessid.manual', dtype='str')\n",
    "\n",
    "for subject in subjects:\n",
    "    \n",
    "    print subject,\n",
    "    \n",
    "    for hemi in ['lh','rh']:\n",
    "        \n",
    "        subj_dir = os.path.join(mri_dir, subject, 'msit_001', 'afMSIT.6.0.5.%s' %hemi)\n",
    "        \n",
    "        ## Load constants.\n",
    "        design_matrix = np.loadtxt(os.path.join(subj_dir, 'X.dat'))\n",
    "        mean_signal = nib.load(os.path.join(subj_dir, 'betaconstant.nii.gz')).get_data()\n",
    "        \n",
    "        ## Iteratively compute PSC across conditions.\n",
    "        for n, con in enumerate(['Neu','Int']):\n",
    "            \n",
    "            ## Load contrast.\n",
    "            ces = nib.load(os.path.join(subj_dir, 'afMSIT.%s.par' %con, 'ces.nii.gz'))\n",
    "            affine = ces.affine\n",
    "            ces = ces.get_data()\n",
    "            \n",
    "            ## Compute PSC.\n",
    "            sf = design_matrix.max(axis=0)[n]\n",
    "            psc = ces * sf / mean_signal * 100.\n",
    "            \n",
    "            ## Save.\n",
    "            psc = nib.Nifti1Image(psc, affine)\n",
    "            nib.save(psc, os.path.join(subj_dir, 'afMSIT.%s.par' %con, 'psc.nii.gz'))\n",
    "            \n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Group Contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from statsmodels.api import WLS\n",
    "mri_dir = '/space/lilli/4/users/DARPA-MSIT'\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "subjects = np.loadtxt('scripts/fsfast/sessid.manual', dtype='str')\n",
    "\n",
    "for hemi in ['lh','rh']:\n",
    "    \n",
    "    print 'Starting analysis of %s.' %hemi\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Iteratively load data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Loading subjects data.',\n",
    "    \n",
    "    psc, var = [], []\n",
    "    for subject in subjects:\n",
    "        \n",
    "        ## Define subject directory.\n",
    "        subj_dir = os.path.join(mri_dir, subject, 'msit_001', 'afMSIT.6.0.5.%s' %hemi)\n",
    "        \n",
    "        for con in ['Neu','Int']:\n",
    "            \n",
    "            ## Load percent signal change.\n",
    "            f = os.path.join(subj_dir, 'afMSIT.%s.par' %con, 'psc.nii.gz')\n",
    "            psc.append( nib.load(f).get_data() )\n",
    "            \n",
    "            ## Load contrast variance.\n",
    "            f = os.path.join(subj_dir, 'afMSIT.%s.par' %con, 'cesvar.nii.gz')\n",
    "            obj = nib.load(f)\n",
    "            var.append( obj.get_data() )\n",
    "    \n",
    "    ## Concatenate contrasts.\n",
    "    psc = np.concatenate(psc, axis=-1).squeeze()\n",
    "    var = np.concatenate(var, axis=-1).squeeze()\n",
    "    affine = obj.affine\n",
    "    print 'Finished.'\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Iteratively perform statistics.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Compute contrast statistics.',\n",
    "    \n",
    "    ## Preallocate arrays.\n",
    "    n_vert, n_cons = psc.shape\n",
    "    Fvals, pvals = np.zeros((n_vert, 3)), np.zeros((n_vert, 3))\n",
    "\n",
    "    ## Generate design matrix.\n",
    "    X = np.zeros((n_cons,2))\n",
    "    X[::2,0] = 1               # Neutral contrasts.\n",
    "    X[1::2,1] = 1              # Interference contrasts.\n",
    "\n",
    "    ## Generate weights.\n",
    "    W = 1. / np.power(var, 2)\n",
    "\n",
    "    ## Iterate over vertices.\n",
    "    contrasts = [[1,0],[0,1],[-1,1]]\n",
    "    for n in range(n_vert):\n",
    "\n",
    "        ## Check if no data available.\n",
    "        if np.any(np.isinf(W[n])): continue\n",
    "\n",
    "        ## Fit model.\n",
    "        model = WLS(psc[n], X, W[n]).fit()        \n",
    "        \n",
    "        ## Determine signs.\n",
    "        signs = np.zeros(3)\n",
    "        signs[:2] = np.sign(model.params)\n",
    "        signs[-1] = np.sign(np.diff(model.params))\n",
    "        \n",
    "        ## Compute contrasts.\n",
    "        for m, c in enumerate(contrasts):\n",
    "\n",
    "            contrast = model.f_test(c)\n",
    "            Fvals[n,m] = contrast.fvalue.max() * signs[m]\n",
    "            pvals[n,m] = contrast.pvalue.max() * signs[m]\n",
    "\n",
    "    ## Transform p-values.\n",
    "    pvals = -np.log10(np.abs(pvals)) * np.sign(pvals)\n",
    "    print 'Finished.'\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Save results.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    print 'Saving results.',\n",
    "    \n",
    "    for n, contrast in enumerate(['neu','int','diff']):\n",
    "        \n",
    "        ## Make out directory.\n",
    "        out_dir = 'fmri/afMSIT.%s.%s' %(contrast,hemi)\n",
    "        if not os.path.isdir(out_dir): os.makedirs(out_dir)\n",
    "        \n",
    "        ## Save F-statistics.\n",
    "        F = Fvals[:,n]\n",
    "        for _ in range(2): F = F[:,np.newaxis]\n",
    "        obj = nib.Nifti1Image(F, affine)\n",
    "        nib.save(obj, os.path.join(out_dir,'F.mgz'))\n",
    "        \n",
    "        ## Save p-values.\n",
    "        p = pvals[:,n]\n",
    "        for _ in range(2): p = p[:,np.newaxis]\n",
    "        obj = nib.Nifti1Image(p, affine)\n",
    "        nib.save(obj, os.path.join(out_dir,'sig.mgz'))\n",
    "            \n",
    "    print 'Finished.'\n",
    "    \n",
    "print 'Done.'"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "nav_menu": {},
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "387px",
    "left": "9px",
    "right": "1553px",
    "top": "113px",
    "width": "184px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "597px",
   "left": "0px",
   "right": "1198.8px",
   "top": "134px",
   "width": "338px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
